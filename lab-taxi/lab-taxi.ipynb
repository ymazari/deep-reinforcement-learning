{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from agent_epsilon import AgentWithEpsilon\n",
    "from monitor import interact\n",
    "import gym\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yacine/.pyenv/versions/3.8.5/lib/python3.8/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env, num_episodes)\n",
    "avg_rewards, best_avg_reward = interact(env, agent, num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_avg_reward(alpha, gamma):\n",
    "    agent = Agent(env, num_episodes, alpha, gamma)\n",
    "    _, best_avg_reward = interact(env, agent, num_episodes)\n",
    "    return best_avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |   gamma   |\n",
      "-------------------------------------------------\n",
      "Episode 20000/20000 || Best average reward 4.8319\n",
      "\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 4.83    \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 4.7388\n",
      "\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 4.73    \u001b[0m | \u001b[0m 0.2668  \u001b[0m | \u001b[0m 0.8602  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward -17.872\n",
      "\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-17.87   \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.6512  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward -123.34\n",
      "\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-123.3   \u001b[0m | \u001b[0m 0.1587  \u001b[0m | \u001b[0m 0.5462  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward -20.467\n",
      "\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-20.46   \u001b[0m | \u001b[0m 0.1435  \u001b[0m | \u001b[0m 0.6844  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 5.3821\n",
      "\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 5.38    \u001b[0m | \u001b[95m 0.2043  \u001b[0m | \u001b[95m 0.9661  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 4.7923\n",
      "\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 4.79    \u001b[0m | \u001b[0m 0.1597  \u001b[0m | \u001b[0m 0.8144  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 5.8498\n",
      "\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 5.84    \u001b[0m | \u001b[95m 0.444   \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward -68.944\n",
      "\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-68.94   \u001b[0m | \u001b[0m 0.2583  \u001b[0m | \u001b[0m 0.5807  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 5.5698\n",
      "\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 5.56    \u001b[0m | \u001b[0m 0.4973  \u001b[0m | \u001b[0m 0.82    \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward -23.717\n",
      "\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-23.71   \u001b[0m | \u001b[0m 0.2729  \u001b[0m | \u001b[0m 0.676   \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward -8.0576\n",
      "\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-8.05    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.6259  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 5.1833\n",
      "\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 5.18    \u001b[0m | \u001b[0m 0.3213  \u001b[0m | \u001b[0m 0.851   \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward -24.397\n",
      "\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-24.39   \u001b[0m | \u001b[0m 0.1594  \u001b[0m | \u001b[0m 0.7033  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 5.6215\n",
      "\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 5.62    \u001b[0m | \u001b[0m 0.3236  \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 5.0183\n",
      "\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 5.01    \u001b[0m | \u001b[0m 0.2015  \u001b[0m | \u001b[0m 0.9664  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 0.78921\n",
      "\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.78    \u001b[0m | \u001b[0m 0.4063  \u001b[0m | \u001b[0m 0.7356  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward -98.666\n",
      "\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-98.66   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 5.1543\n",
      "\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 5.15    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.9258  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 0.02449\n",
      "\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.02    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.7184  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 5.3514\n",
      "\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 5.35    \u001b[0m | \u001b[0m 0.4055  \u001b[0m | \u001b[0m 0.9085  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward -0.2799\n",
      "\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.2     \u001b[0m | \u001b[0m 0.1007  \u001b[0m | \u001b[0m 0.7806  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 6.1713\n",
      "\n",
      "| \u001b[95m 23      \u001b[0m | \u001b[95m 6.17    \u001b[0m | \u001b[95m 0.1878  \u001b[0m | \u001b[95m 0.9164  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 6.0975\n",
      "\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 6.09    \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 5.1814\n",
      "\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 5.18    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 5.7278\n",
      "\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 5.72    \u001b[0m | \u001b[0m 0.306   \u001b[0m | \u001b[0m 0.9292  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 4.4544\n",
      "\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 4.45    \u001b[0m | \u001b[0m 0.4166  \u001b[0m | \u001b[0m 0.8246  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 2.84265\n",
      "\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 2.84    \u001b[0m | \u001b[0m 0.2846  \u001b[0m | \u001b[0m 0.744   \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 4.9541\n",
      "\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 4.95    \u001b[0m | \u001b[0m 0.3403  \u001b[0m | \u001b[0m 0.7769  \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "pbounds = {'alpha': (0.1, 0.5), 'gamma': (0.5, 1.0)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=best_avg_reward,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "optimizer.probe(\n",
    "    params={'alpha': 0.1, 'gamma': 0.9},\n",
    "    lazy=True,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=3,\n",
    "    n_iter=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': 6.17, 'params': {'alpha': 0.18779344372845774, 'gamma': 0.9164131198523765}}\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_avg_reward_epsilon(epsilon, alpha, gamma):\n",
    "    agent = AgentWithEpsilon(env, num_episodes, epsilon, alpha, gamma)\n",
    "    _, best_avg_reward = interact(env, agent, num_episodes)\n",
    "    return best_avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |  epsilon  |   gamma   |\n",
      "-------------------------------------------------------------\n",
      "Episode 20000/20000 || Best average reward 5.294\n",
      "\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 5.29    \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 5.668\n",
      "\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 5.6     \u001b[0m | \u001b[95m 0.1454  \u001b[0m | \u001b[95m 0.0977  \u001b[0m | \u001b[95m 0.8644  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 6.676\n",
      "\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 6.67    \u001b[0m | \u001b[95m 0.2406  \u001b[0m | \u001b[95m 0.07368 \u001b[0m | \u001b[95m 0.8998  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 7.625\n",
      "\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 7.62    \u001b[0m | \u001b[95m 0.3582  \u001b[0m | \u001b[95m 0.04731 \u001b[0m | \u001b[95m 0.853   \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 9.131\n",
      "\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 9.13    \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 0.9948  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 5.688\n",
      "\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 5.68    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.878\n",
      "\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 8.87    \u001b[0m | \u001b[0m 0.4919  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.9619  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.633\n",
      "\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 8.63    \u001b[0m | \u001b[0m 0.4234  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.829\n",
      "\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 8.82    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.7739  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 9.077\n",
      "\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 9.0     \u001b[0m | \u001b[0m 0.4707  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.6507  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 4.996\n",
      "\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 4.99    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.5425  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.899\n",
      "\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 8.89    \u001b[0m | \u001b[0m 0.3936  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.7117  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.728\n",
      "\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 8.72    \u001b[0m | \u001b[0m 0.4974  \u001b[0m | \u001b[0m 0.0142  \u001b[0m | \u001b[0m 0.7744  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.835\n",
      "\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 8.83    \u001b[0m | \u001b[0m 0.259   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.6027  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.726\n",
      "\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 8.72    \u001b[0m | \u001b[0m 0.4415  \u001b[0m | \u001b[0m 0.01507 \u001b[0m | \u001b[0m 0.5809  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.491\n",
      "\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 8.49    \u001b[0m | \u001b[0m 0.1218  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.691\n",
      "\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 8.69    \u001b[0m | \u001b[0m 0.3121  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 2.896\n",
      "\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 2.8     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.61    \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 9.094\n",
      "\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 9.09    \u001b[0m | \u001b[0m 0.3536  \u001b[0m | \u001b[0m 0.01097 \u001b[0m | \u001b[0m 0.6082  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.081\n",
      "\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 8.08    \u001b[0m | \u001b[0m 0.2289  \u001b[0m | \u001b[0m 0.01524 \u001b[0m | \u001b[0m 0.5001  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.976\n",
      "\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 8.97    \u001b[0m | \u001b[0m 0.2765  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.7254  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 5.639\n",
      "\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 5.63    \u001b[0m | \u001b[0m 0.3261  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.6841  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.872\n",
      "\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 8.87    \u001b[0m | \u001b[0m 0.282   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.161\n",
      "\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 8.16    \u001b[0m | \u001b[0m 0.2703  \u001b[0m | \u001b[0m 0.03413 \u001b[0m | \u001b[0m 0.6746  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 6.043\n",
      "\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 6.04    \u001b[0m | \u001b[0m 0.4627  \u001b[0m | \u001b[0m 0.09269 \u001b[0m | \u001b[0m 0.9147  \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 9.087\n",
      "\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 9.08    \u001b[0m | \u001b[0m 0.1693  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.875\n",
      "\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 8.87    \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 8.692\n",
      "\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 8.69    \u001b[0m | \u001b[0m 0.4034  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
      "Episode 20000/20000 || Best average reward 9.061\n",
      "\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 9.06    \u001b[0m | \u001b[0m 0.2445  \u001b[0m | \u001b[0m 0.01017 \u001b[0m | \u001b[0m 0.8344  \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "pbounds = {'epsilon': (0.01, 0.1), 'alpha': (0.1, 0.5), 'gamma': (0.5, 1.0)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=best_avg_reward_epsilon,\n",
    "    pbounds=pbounds,\n",
    "    random_state=47\n",
    ")\n",
    "\n",
    "optimizer.probe(\n",
    "    params={'epsilon': 0.1, 'alpha': 0.1, 'gamma': 0.9},\n",
    "    lazy=True,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=3,\n",
    "    n_iter=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': 9.13, 'params': {'alpha': 0.5, 'epsilon': 0.01, 'gamma': 0.9947502701537442}}\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
